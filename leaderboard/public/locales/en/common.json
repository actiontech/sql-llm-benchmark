{
  "seo": {
    "title": "SQL Capability Leaderboard for LLMs",
    "description": "SCALE - SQL Capability Leaderboard for LLMs, evaluating and comparing the performance of mainstream large models in SQL capabilities.",
    "keywords": "SCALE, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation",
    "ranking_page": {
      "title": "SCALE - SQL LLM Leaderboard - {{month}}",
      "description": "SCALE - View the SQL LLM capability leaderboard for {{month}}, evaluating and comparing the performance of mainstream large models in SQL capabilities.",
      "keywords": "SCALE, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation, {{month}}"
    },
    "model_detail_page": {
      "title": "SCALE - {{modelName}} - SQL LLM Details",
      "description": "SCALE - Detailed evaluation report on {{modelName}}'s SQL capabilities, including SQL optimization, dialect conversion, and SQL understanding.",
      "keywords": "SCALE, {{modelName}}, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation"
    },
    "compare_page": {
      "title": "Model Comparison - {{month}}",
      "description": "Comparing the SQL capabilities of {{count}} large language models."
    }
  },
  "ranking": {
    "title": "SCALE",
    "description_part1": "This leaderboard evaluates the SQL capabilities of major large language models. For more details, check out our",
    "description_part2": " or",
    "description_part3_trigger": "submit your own evaluation report",
    "current_month_realtime": "Real-time",
    "full_description": "The Large Model SQL Capability Leaderboard (SCALE) reveals the true proficiency of large models in SQL. SCALE comprehensively evaluates Large Language Models' (LLMs) core SQL capabilities through scientific, rigorous assessment, focusing on three critical dimensions: SQL Optimization (enhancing query efficiency and performance), Dialect Conversion (enabling seamless cross-database platform migration), and Deep SQL Comprehension (accurately parsing complex logic and user intent). To authentically reflect real-world database operation performance, we established a multi-dimensional, multi-metric evaluation system employing strict testing with real-world cases across varying difficulty levels. Each test case carries scientifically calibrated weights based on technical complexity and practical value (higher difficulty = greater weight), ensuring final scores precisely measure models' comprehensive performance in high-value, high-challenge tasks. Through rigorous testing and weighted scoring, SCALE provides developers, database administrators, and enterprise technology decision-makers with authoritative, objective benchmarks, clearly delineating models' relative strengths in SQL processing to advance intelligent database application development and implementation."
  },
  "common": {
    "select_evaluation_type": "Select Evaluation Type",
    "dimension_ranking": "Dimension Ranking",
    "dimension_ranking_desc": "View comprehensive rankings of models in SQL optimization, dialect conversion, SQL understanding and other dimensions",
    "indicator_ranking": "Indicator Ranking",
    "indicator_ranking_desc": "View detailed score rankings of models on specific indicators",
    "latest_data": "Latest Data",
    "historical_data": "Historical Data",
    "no_data_available": "No Data Available",
    "search_models": "Search Models",
    "items": "items",
    "reset": "Reset",
    "back_to_ranking": "Back to Ranking",
    "back_to_detail": "Back to Detail",
    "back": "Back",
    "prev": "Previous",
    "next": "Next",
    "select_all": "Select All",
    "loading": "Loading",
    "unknown_vendor": "Other"
  },
  "table": {
    "rank": "Rank",
    "creator": "Creator",
    "model": "Model",
    "releaseDate": "Release Date",
    "type": "Type",
    "type_chat": "Chat",
    "type_application": "Baseline",
    "type_chat_thinking": "Chat(Thinking)",
    "parameters": "Parameters",
    "dialect_conversion": "Dialect Conversion",
    "sql_optimization": "SQL Optimization",
    "sql_understanding": "SQL Understanding",
    "organization": "Organization",
    "website": "Website",
    "details": "Details",
    "view_details": "detail",
    "model_score": "model score",
    "default_sort_tooltip": "Default Sorting",
    "no_data": "N/A",
    "dimension": "Dimension",
    "indicator": "Indicator",
    "new_model_tag": "NEW",
    "select": "Select"
  },
  "indicator": {
    "execution_accuracy.jsonl": "Execution Accuracy",
    "explain_detection.jsonl": "Explain Detection",
    "sql_identification.jsonl": "Type Identification",
    "syntax_error_detection.jsonl": "Syntax Error Detection",
    "index_advice.jsonl": "Index Advice",
    "logical_equivalence.jsonl": "Logical Equivalence",
    "optimization_depth.jsonl": "Optimization Depth",
    "rule_adherence.jsonl": "Rule Adherence",
    "big_sql_conversion.jsonl": "Big SQL Conversion",
    "China-made_database.jsonl": "China-made Database"
  },
  "detail": {
    "abilityScores": "Ability Scores"
  },
  "evaluation_cases": {
    "title": "Evaluation Case Report",
    "formula_button": "Ranking Rules",
    "select_dimension": "Select Ability Dimension",
    "no_data_found": "No evaluation case data found for this dimension.",
    "indicator_name": "Indicator Name",
    "indicator_weight": "Indicator Weight",
    "evaluation_type": "Evaluation Type",
    "total_cases": "Total Cases",
    "passed_cases": "Passed Cases",
    "failed_cases": "Failed Cases",
    "pass_rate": "Pass Rate",
    "average_score": "Average Score",
    "details": "Case Details",
    "view_cases": "View Cases",
    "case_details": "Case Details",
    "case_id": "Case ID",
    "case_description": "Case Description",
    "case_weight": "Case Weight",
    "case_content": "Case Content",
    "model_answers": "Model Answers",
    "mode_question": "Mode Question",
    "expected_output": "Expected Output",
    "actual_output": "Actual Output",
    "evaluation_result": "Evaluation Result",
    "score": "Score",
    "reason": "Reason",
    "formula_tip": {
      "title": "Ability Score Calculation Logic",
      "basic_elements": {
        "title": "Basic Elements",
        "item1": "Each ability contains multiple evaluation indicators",
        "item2": "Each indicator contains multiple test cases",
        "item3": "Each case has a difficulty level (1-3)"
      },
      "weight_settings": {
        "title": "Weight Settings",
        "item1": "Indicator Weight: Reflects importance (higher = more important)",
        "item2": "Difficulty Weight: Level 1=1pt, Level 2=2pt, Level 3=3pt"
      },
      "scoring_steps": {
        "title": "Scoring Steps",
        "item1": "Case Score = Difficulty Weight √ó Correctness (1 for correct, 0 for wrong)",
        "item2": "Indicator Score = Sum of all case scores in the indicator",
        "item3": "Total Ability Score = ‚àë(Indicator Score √ó Indicator Weight)",
        "item4": "Max Possible Score = ‚àë(Sum of case difficulty weights per indicator √ó Indicator Weight)",
        "item5": "Final Ability Score = (Total Ability Score √∑ Max Possible Score) √ó 100"
      },
      "special_cases": {
        "title": "Special Cases",
        "item1": "Score=0 if no test cases",
        "item2": "Indicators without configured weights are excluded"
      },
      "example": {
        "title": "Example",
        "item1": "Indicator A (weight 4): 3 cases (difficulty 1/2/3 all correct) ‚Üí Indicator Score= 1√ó1 + 2√ó1 + 3√ó1 = 6",
        "item2": "Indicator B (weight 2): 2 cases (difficulty 2/3 all correct) ‚Üí Indicator Score= 2√ó1 + 3√ó1 = 5",
        "item3": "Total Score = 6√ó4 + 5√ó2 = 34",
        "item4": "Max Score = (1+2+3)√ó4 + (2+3)√ó2 = 34",
        "item5": "Final Score = 34/34√ó100 = 100 points"
      }
    },
    "rule_id": "Rule ID"
  },
  "actions": {
    "zh": "Chinese Eval",
    "en": "English Eval",
    "searchPlaceholder": "Search model name",
    "export": "Export",
    "back": "Back to List",
    "toggle_language": "English",
    "expand": "View All",
    "collapse": "Collapse",
    "submit_report": "Submit Your Report",
    "close": "Close"
  },
  "compare": {
    "toggle_compare_mode": "Model Compare",
    "exit_compare_mode": "Exit Compare",
    "selected_models_count": "Selected {{count}}/5 models",
    "start_compare": "Start Compare ({{count}} models)",
    "compare_mode_tip": "Compare mode enabled: Click table rows or checkboxes to select models, up to 5 models for comparison",
    "checkbox_tooltip": {
      "select": "Select this model for comparison",
      "deselect": "Deselect this model",
      "max_reached": "Maximum 5 models can be selected for comparison"
    },
    "minimum_models_required": "Please select at least 2 models for comparison",
    "back_to_ranking": "Back to Leaderboard",
    "analysis_title": "Model Comparison Analysis",
    "compare_models_label": "Compare Models:",
    "add_model": "Add Model",
    "max_models_reached": "Maximum 5 models can be selected for comparison",
    "switch_language": "Switch Language",
    "loading_data": "Loading comparison data...",
    "chart_types": {
      "radar": "Dimension Capability Radar Chart",
      "bar": "Detailed Score Bar Chart",
      "heatmap": "Heatmap"
    },
    "capability_filter": {
      "all": "All Dimensions",
      "sql_optimization": "SQL Optimization",
      "dialect_conversion": "Dialect Conversion",
      "sql_understanding": "SQL Understanding"
    },
    "chart_titles": {
      "capability_comparison": "Capability Comparison Visualization",
      "detailed_comparison": "Detailed Data Comparison",
      "model_capability_radar": "Model Capability Radar Chart",
      "indicator_comparison": "Indicator Capability Comparison",
      "model_indicator_heatmap": "Model Indicator Heatmap"
    },
    "export_data": "Export Data",
    "evaluation_item": "Evaluation Item",
    "model_comparison_data": "Model Comparison Data",
    "all_models_unavailable": "All selected models are unavailable in {{month}}, redirected to leaderboard",
    "insufficient_models_available": "Only {{available}} models available in {{month}}, insufficient for comparison, redirected to leaderboard",
    "some_models_unavailable": "{{count}} models are unavailable in {{month}}, comparing with remaining {{available}} models",
    "month_switch_error": "Error occurred while switching month, please try again later",
    "month_data_not_found": "Data for {{month}} does not exist",
    "average_score_label": "Avg Score",
    "rank_label": "Rank",
    "score_unit": "pts",
    "indicator_fallback": "Indicator",
    "indicator_comparison_suffix": " Indicator Comparison",
    "heatmap_suffix": " Heatmap",
    "high_score": "High",
    "low_score": "Low"
  },
  "submission_guide": {
    "title": "Submission Guidelines",
    "intro": "We welcome community contributions! To ensure the quality and consistency of the leaderboard, please follow these guidelines when submitting your model's evaluation report.",
    "cta_button": "Contribute on GitHub",
    "req_title": "Submission Requirements",
    "req1": "Report Integrity: All three reports generated by the evaluator must be submitted in full and the evaluation time (model score report, case run report, evaluation process report) must be stated in the PR.",
    "req2": "Complete Model Configuration: The model configuration in `llm_config.py` under `TARGET_LLM_CONFIG` must be complete, including all display-related fields.",
    "req3": "Standard Datasets: If using our standard dataset, you must perform a full evaluation across all three dimensions and all indicators.",
    "req4": "Custom Datasets: If using a custom dataset, ensure it follows the required format. A full evaluation across all three dimensions and indicators is still required.",
    "req5": "No Duplicate Models: We do not accept model evaluation reports that are already on the ranking list for the month you are evaluating the model.",
    "req6": "Further Reading: For detailed instructions on using the evaluator and other information, please refer to the `README.md` in our GitHub repository."
  },
  "footer": {
    "copyright": "¬© Copyright 2025 Shanghai Action Information Technology Co., Ltd. All rights reserved.",
    "beian": "Ê≤™ICPÂ§á12003970Âè∑-5"
  },
  "config": {
    "language": "üåê Language"
  },
  "log_info": {
    "title": "Evaluation Process",
    "select_file": "Select Log File",
    "no_logs_found": "No log files found for this dimension.",
    "no_logs_for_dimension": "No evaluation process log data found for this dimension.",
    "logContent": "Log Content"
  },
  "indicator_ranking": {
    "title": "Model Indicator Score Ranking",
    "dimension_filter": "Dimension Filter",
    "indicator_filter": "Indicator Filter",
    "select_indicator": "Select Indicator",
    "available_indicators": "Available Indicators",
    "current_indicator": "Current Indicator",
    "table_title": "Indicator Ranking",
    "export_csv": "Export CSV",
    "total_models": "Total Models",
    "highest_score": "Highest Score",
    "average_score": "Average Score",
    "no_data": "No Data Available",
    "compare_models": "Compare Models",
    "select_models": "Select Models",
    "start_compare": "Start Compare"
  },
  "dialect_conversion": {
    "logical_equivalence": "Logical Equivalence",
    "syntax_error_detection": "Syntax Error Detection",
    "big_sql_conversion": "Big SQL Conversion",
    "China-made_database": "China-made Database"
  },
  "sql_optimization": {
    "logical_equivalence": "Logical Equivalence",
    "optimization_depth": "Optimization Depth",
    "index_advice": "Index Advice",
    "rule_adherence": "Rule Adherence",
    "syntax_error_detection": "Syntax Error Detection"
  },
  "sql_understanding": {
    "execution_accuracy": "Execution Accuracy",
    "explain_detection": "Explain Detection",
    "sql_identification": "Type Identification",
    "syntax_error_detection": "Syntax Error Detection"
  },
  "log_file": {
    "dialect_conversion": {
      "logical_equivalence": "Logical Equivalence",
      "syntax_error_detection": "Syntax Error Detection",
      "big_sql_conversion": "Big SQL Conversion",
      "China-made_database": "China-made Database"
    },
    "sql_optimization": {
      "logical_equivalence": "Logical Equivalence",
      "optimization_depth": "Optimization Depth",
      "index_advice": "Index Advice",
      "rule_adherence": "Rule Adherence",
      "syntax_error_detection": "Syntax Error Detection"
    },
    "sql_understanding": {
      "execution_accuracy": "Execution Accuracy",
      "explain_detection": "Explain Detection",
      "sql_identification": "Type Identification",
      "syntax_error_detection": "Syntax Error Detection"
    }
  },
  "blog": {
    "title": "Scale News",
    "description": "Explore in-depth analysis and technical insights on SQL capability evaluation for large language models",
    "noPosts": "No posts available",
    "notFound": "Post not found",
    "backToList": "Back to List",
    "readMore": "Read More",
    "viewBlog": "View Scale News",
    "search": "Search",
    "searchPlaceholder": "Search articles...",
    "previous": "Previous",
    "next": "Next"
  },
  "news": {
    "title": "News & Updates",
    "description": "Stay updated with the latest SCALE developments and industry insights",
    "noPosts": "No news available",
    "notFound": "News not found",
    "backToList": "Back to List",
    "readMore": "Read More",
    "viewNews": "View News",
    "search": "Search",
    "searchPlaceholder": "Search news...",
    "previous": "Previous",
    "next": "Next"
  },
  "nav": {
    "home": "LLM Leaderboard",
    "home_label": "Leaderboard",
    "blog": "blog",
    "news": "News",
    "about": "About Us",
    "github": "Visit GitHub Repository",
    "contribute_evaluation": "Contribute Evaluation",
    "online_evaluation": "Custom Evaluation"
  },
  "evaluation": {
    "modal_title": "Create Online Evaluation",
    "create_button": "Create Evaluation",
    "submit_button": "Submit Evaluation Request",
    "confirm_title": "Please Confirm the Following Information",
    "task_id": "Task ID",
    "estimated_time": "Estimated Completion Time",
    "result_email_hint": "Results will be sent to",
    "token_estimate": "Token",
    "view_dataset": "View Dataset",
    "selected_datasets": "Selected Datasets",
    "estimated_tokens": "Estimated Total Tokens",
    "dataset_description": "{{count}} test cases",
    "custom_model": {
      "title": "Custom Model Evaluation",
      "page_title": "Custom Model Evaluation",
      "description": "Submit your model information, and we will conduct a comprehensive evaluation using SCALE evaluation datasets",
      "goal": "Comprehensively assess your LLM's capabilities in core SQL scenarios including optimization, dialect conversion, and understanding",
      "desc_1": "SQL capability evaluation for models",
      "desc_2": "Support user-defined models",
      "desc_3": "Support all SCALE evaluation datasets",
      "desc_4": "Evaluation results via email notification",
      "steps": {
        "model_info": "Model Information",
        "dataset_select": "Select Datasets",
        "email_and_confirm": "Confirmation & Email"
      },
      "form": {
        "model_name": "Model Name",
        "model_name_placeholder": "e.g., GPT-4-Turbo",
        "model_name_required": "Please enter model name",
        "api_url": "Model API URL",
        "api_url_required": "Please enter API URL",
        "api_url_invalid": "Please enter a valid URL",
        "api_key": "Model API Key",
        "api_key_required": "Please enter API Key",
        "api_key_hint": "üîí Your API Key will not be stored and will only be used for this evaluation",
        "max_concurrency": "Maximum Concurrency",
        "max_concurrency_required": "Please enter concurrency",
        "max_concurrency_range": "Concurrency range 1-20",
        "result_email": "Evaluation Result Email",
        "result_email_required": "Please enter email",
        "result_email_invalid": "Please enter a valid email address",
        "verification_code": "Email Verification Code",
        "verification_code_required": "Please enter verification code",
        "verification_code_placeholder": "Enter 6-digit code",
        "send_verification_code": "Send Code",
        "resend_verification_code": "Resend in {{seconds}}s",
        "verification_code_sent": "Code sent",
        "verification_code_hint": "Code valid for 5 minutes, do not share",
        "model_call_info": "Model Call Instructions (Optional)",
        "model_call_info_placeholder": "Please describe how to call your model, for example:\n‚Ä¢ Model provider (e.g., OpenAI, Google, Azure OpenAI, etc.)\n‚Ä¢ Specific model version (e.g., 2024-04-09)\n‚Ä¢ Other notes\nYou can also paste code examples directly. We will manually configure and call your model based on this information.",
        "model_call_info_hint": "‚ö†Ô∏è Please ensure the information provided is detailed and accurate enough for us to successfully call your model",
        "datasets": "Evaluation Datasets"
      },
      "warning": {
        "balance_title": "Please ensure sufficient account balance",
        "balance_description": "Full dataset contains approximately {{tokens}} tokens. Evaluation will consume your API quota, please ensure sufficient account balance"
      },
      "dataset_select_hint": "Please select evaluation datasets to use",
      "email_hint": "Evaluation report will be sent to this email upon completion",
      "error": {
        "no_dataset_selected": "Please select at least one dataset"
      }
    },
    "custom_dataset": {
      "title": "Custom Dataset Evaluation",
      "page_title": "Custom Dataset Evaluation",
      "description": "Upload your custom dataset to evaluate with leaderboard models",
      "goal": "Horizontally compare mainstream LLMs' SQL capabilities on your proprietary business datasets to provide scientific basis for model selection",
      "desc_1": "Use custom evaluation dataset",
      "desc_2": "Support evaluation with all leaderboard models",
      "desc_3": "Commercial cooperation prioritized",
      "desc_4": "Evaluation results via email notification",
      "steps": {
        "business_info": "Business Information",
        "dataset_config": "Dataset Configuration",
        "confirm_and_email": "Confirmation & Email"
      },
      "form": {
        "name": "Your Name",
        "name_placeholder": "John Doe",
        "name_required": "Please enter your name",
        "phone": "Phone Number",
        "phone_required": "Please enter phone number",
        "phone_invalid": "Please enter a valid phone number",
        "company": "Company Name",
        "company_placeholder": "XX Technology Co., Ltd.",
        "company_required": "Please enter company name",
        "email": "Enterprise Email",
        "email_required": "Please enter enterprise email",
        "email_invalid": "Please enter a valid email address",
        "dataset_file": "Evaluation Dataset",
        "dataset_name": "Dataset Name",
        "dataset_name_placeholder": "Enter dataset name (optional)",
        "dataset_name_tooltip": "Specify a name for your evaluation dataset for easy identification",
        "dataset_description": "Dataset Description",
        "dataset_description_placeholder": "Please briefly describe your evaluation dataset...",
        "notes": "Notes",
        "notes_placeholder": "Special considerations for evaluation...",
        "judge_mode": "Judge Mode",
        "judge_mode_required": "Please select judge mode",
        "target_prompt": "Target Model Prompt",
        "target_prompt_placeholder": "System prompt for the evaluated model (optional)...",
        "judge_prompt": "Judge Model Prompt",
        "judge_prompt_placeholder": "System prompt for the judge model (optional)...",
        "model_name": "Model Name",
        "model_name_required": "Please enter model name",
        "api_url_required": "Please enter API URL",
        "api_key_required": "Please enter API Key"
      },
      "business_info_hint": "Business information is only used for contact and service, we will keep it strictly confidential",
      "email_hint": "Evaluation report will be sent to this email upon completion",
      "model_select_title": "Select Evaluation Models",
      "leaderboard_models": "Leaderboard Models",
      "custom_models": "Custom Models",
      "custom_models_request": "Other Models You'd Like Supported",
      "custom_models_request_tooltip": "Let us know if you'd like us to support other models",
      "custom_models_request_placeholder": "Models you'd like to evaluate with this dataset. Separate multiple models with semicolons (;), e.g.: gpt-5;claude-4;gemini-ultra",
      "add_custom_model": "Add Custom Model",
      "evaluation_config": "Evaluation Configuration",
      "judge_mode": {
        "objective": "Objective",
        "subjective": "Subjective",
        "mixed": "Hybrid",
        "hybrid": "Hybrid",
        "description_title": "Evaluation Method Description:",
        "objective_detail": {
          "title": "Objective Evaluation",
          "feature": "Deterministic answers with unique standards",
          "scenario": "SQL syntax correctness detection, execution accuracy verification, and other evaluations with clear pass/fail criteria",
          "method": "Direct comparison between model output and preset standard answers through automated programs, results in pass/fail"
        },
        "subjective_detail": {
          "title": "Subjective Evaluation",
          "feature": "Open-ended answers relying on semantic understanding",
          "scenario": "Logical equivalence determination, SQL dialect conversion, and other evaluations without preset fixed answers",
          "method": "Multiple high-performance judge models independently evaluate the correctness and reasonableness of model output, final conclusion determined by majority voting"
        },
        "mixed_detail": {
          "title": "Hybrid Evaluation",
          "feature": "Semi-open answers with clear evaluation dimensions",
          "scenario": "SQL optimization effectiveness evaluation and other assessments with preset evaluation rules but non-unique answers, guided by rule-driven judge models",
          "method": "Based on predefined rule sets, multiple judge models use majority voting to analyze whether model output triggers target optimization rules, comprehensive scoring based on rule coverage and trigger accuracy"
        },
        "feature_label": "Feature: ",
        "scenario_label": "Applicable Scenarios: ",
        "method_label": "Judgment Method: "
      },
      "upload": {
        "click_or_drag": "Click or drag file to this area",
        "hint": "Support .jsonl or .csv format, file size not exceeding 20MB"
      },
      "contact": "Contact",
      "evaluation_models": "Evaluation Models",
      "error": {
        "no_file": "Please upload dataset file",
        "no_model": "Please select at least one model",
        "invalid_file_type": "Only JSONL or CSV format supported",
        "file_too_large": "File size cannot exceed 20MB"
      },
      "fetch_models_failed": "Failed to fetch model list",
      "fetch_models_error": "An error occurred while fetching model list, please try again later",
      "no_models_available": "No leaderboard models available"
    },
    "commercial_info": "We will proactively contact you after the evaluation is completed to provide detailed evaluation reports and professional consulting services. If you have any questions, please feel free to contact us via the email below.",
    "contact_email": "wanghucheng@actionsky.com",
    "success": {
      "task_submitted": "Task submitted successfully!",
      "submit_success": "Submission Successful",
      "estimated_duration": "Estimated completion: ~3 days"
    },
    "error": {
      "submit_failed": "Submission failed",
      "network_error": "Network error, please try again later",
      "fetch_datasets_failed": "Failed to fetch dataset list"
    }
  },
  "pagination": {
    "previous": "Previous",
    "next": "Next",
    "page": "Page {{page}}",
    "total": "Total {{total}} posts"
  },
  "about": {
    "title": "About Us",
    "description": "Learn about the mission and vision of SCALE SQL LLM Leaderboard",
    "section1": {
      "title": "Project Overview",
      "content": "SCALE (SQL Capability Leaderboard for LLMs) is a leaderboard system dedicated to evaluating the SQL capabilities of large language models. We are committed to comprehensively assessing the performance of large language models in core capabilities such as SQL optimization, dialect conversion, and SQL understanding through scientific and rigorous evaluation methods."
    },
    "section2": {
      "title": "Our Mission",
      "content": "To provide developers, database administrators, and enterprise technology decision-makers with authoritative and objective reference standards, clearly presenting the relative advantages of each model in SQL processing, and promoting the technological development and implementation selection of large models in intelligent database applications."
    },
    "section3": {
      "title": "Contact Us",
      "content_before": "If you have any questions or suggestions, please feel free to contact us through our ",
      "content_link": "GitHub repository",
      "content_after": ". We welcome community contributions and feedback to jointly promote the development of SQL LLM evaluation."
    }
  }
}