{
  "seo": {
    "title": "SQL Capability Leaderboard for LLMs",
    "description": "SCALE - SQL Capability Leaderboard for LLMs, evaluating and comparing the performance of mainstream large models in SQL capabilities.",
    "keywords": "SCALE, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation",
    "ranking_page": {
      "title": "SCALE - SQL LLM Leaderboard - {{month}}",
      "description": "SCALE - View the SQL LLM capability leaderboard for {{month}}, evaluating and comparing the performance of mainstream large models in SQL capabilities.",
      "keywords": "SCALE, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation, {{month}}"
    },
    "model_detail_page": {
      "title": "SCALE - {{modelName}} - SQL LLM Details",
      "description": "SCALE - Detailed evaluation report on {{modelName}}'s SQL capabilities, including SQL optimization, dialect conversion, and SQL understanding.",
      "keywords": "SCALE, {{modelName}}, SQL Evaluation, SQL Capability Evaluation, SQL Dialect, SQL Understanding, SQL Optimization, SQL Dialect Conversion, SQL Generation, SQL Conversion, Large Language Model Evaluation"
    },
    "compare_page": {
      "title": "Model Comparison - {{month}}",
      "description": "Comparing the SQL capabilities of {{count}} large language models."
    }
  },
  "ranking": {
    "title": "SCALE",
    "description_part1": "This leaderboard evaluates the SQL capabilities of major large language models. For more details, check out our",
    "description_part2": " or",
    "description_part3_trigger": "submit your own evaluation report",
    "full_description": "The Large Model SQL Capability Leaderboard (SCALE) reveals the true proficiency of large models in SQL. SCALE comprehensively evaluates Large Language Models' (LLMs) core SQL capabilities through scientific, rigorous assessment, focusing on three critical dimensions: SQL Optimization (enhancing query efficiency and performance), Dialect Conversion (enabling seamless cross-database platform migration), and Deep SQL Comprehension (accurately parsing complex logic and user intent). To authentically reflect real-world database operation performance, we established a multi-dimensional, multi-metric evaluation system employing strict testing with real-world cases across varying difficulty levels. Each test case carries scientifically calibrated weights based on technical complexity and practical value (higher difficulty = greater weight), ensuring final scores precisely measure models' comprehensive performance in high-value, high-challenge tasks. Through rigorous testing and weighted scoring, SCALE provides developers, database administrators, and enterprise technology decision-makers with authoritative, objective benchmarks, clearly delineating models' relative strengths in SQL processing to advance intelligent database application development and implementation."
  },
  "common": {
    "select_evaluation_type": "Select Evaluation Type",
    "dimension_ranking": "Dimension Ranking",
    "dimension_ranking_desc": "View comprehensive rankings of models in SQL optimization, dialect conversion, SQL understanding and other dimensions",
    "indicator_ranking": "Indicator Ranking",
    "indicator_ranking_desc": "View detailed score rankings of models on specific indicators",
    "latest_data": "Latest Data",
    "historical_data": "Historical Data",
    "no_data_available": "No Data Available",
    "search_models": "Search Models",
    "items": "items",
    "reset": "Reset",
    "back_to_ranking": "Back to Ranking",
    "back_to_detail": "Back to Detail"
  },
  "table": {
    "rank": "Rank",
    "creator": "Creator",
    "model": "Model",
    "releaseDate": "Release Date",
    "type": "Type",
    "type_chat": "Chat",
    "type_application": "Baseline",
    "type_chat_thinking": "Chat(Thinking)",
    "parameters": "Parameters",
    "dialect_conversion": "Dialect Conversion",
    "sql_optimization": "SQL Optimization",
    "sql_understanding": "SQL Understanding",
    "organization": "Organization",
    "website": "Website",
    "details": "Details",
    "view_details": "detail",
    "model_score": "model score",
    "default_sort_tooltip": "Default Sorting",
    "no_data": "N/A",
    "dimension": "Dimension",
    "indicator": "Indicator",
    "new_model_tag": "NEW"
  },
  "indicator": {
    "execution_accuracy.jsonl": "Execution Accuracy",
    "explain_detection.jsonl": "Explain Detection",
    "sql_identification.jsonl": "Type Identification",
    "syntax_error_detection.jsonl": "Syntax Error Detection",
    "logical_equivalence.jsonl": "Logical Equivalence",
    "optimization_depth.jsonl": "Optimization Depth",
    "rule_adherence.jsonl": "Rule Adherence",
    "big_sql_conversion.jsonl": "Big SQL Conversion",
    "China-made_database.jsonl": "China-made Database"
  },
  "detail": {
    "abilityScores": "Ability Scores"
  },
  "evaluation_cases": {
    "title": "Evaluation Case Report",
    "formula_button": "Ranking Rules",
    "select_dimension": "Select Ability Dimension",
    "no_data_found": "No evaluation case data found for this dimension.",
    "indicator_name": "Indicator Name",
    "indicator_weight": "Indicator Weight",
    "evaluation_type": "Evaluation Type",
    "total_cases": "Total Cases",
    "passed_cases": "Passed Cases",
    "failed_cases": "Failed Cases",
    "pass_rate": "Pass Rate",
    "average_score": "Average Score",
    "details": "Case Details",
    "view_cases": "View Cases",
    "case_details": "Case Details",
    "case_id": "Case ID",
    "case_description": "Case Description",
    "case_weight": "Case Weight",
    "case_content": "Case Content",
    "model_answers": "Model Answers",
    "mode_question": "Mode Question",
    "expected_output": "Expected Output",
    "actual_output": "Actual Output",
    "evaluation_result": "Evaluation Result",
    "score": "Score",
    "reason": "Reason",
    "formula_tip": {
      "title": "Ability Score Calculation Logic",
      "basic_elements": {
        "title": "Basic Elements",
        "item1": "Each ability contains multiple evaluation indicators",
        "item2": "Each indicator contains multiple test cases",
        "item3": "Each case has a difficulty level (1-3)"
      },
      "weight_settings": {
        "title": "Weight Settings",
        "item1": "Indicator Weight: Reflects importance (higher = more important)",
        "item2": "Difficulty Weight: Level 1=1pt, Level 2=2pt, Level 3=3pt"
      },
      "scoring_steps": {
        "title": "Scoring Steps",
        "item1": "Case Score = Difficulty Weight √ó Correctness (1 for correct, 0 for wrong)",
        "item2": "Indicator Score = Sum of all case scores in the indicator",
        "item3": "Total Ability Score = ‚àë(Indicator Score √ó Indicator Weight)",
        "item4": "Max Possible Score = ‚àë(Sum of case difficulty weights per indicator √ó Indicator Weight)",
        "item5": "Final Ability Score = (Total Ability Score √∑ Max Possible Score) √ó 100"
      },
      "special_cases": {
        "title": "Special Cases",
        "item1": "Score=0 if no test cases",
        "item2": "Indicators without configured weights are excluded"
      },
      "example": {
        "title": "Example",
        "item1": "Indicator A (weight 4): 3 cases (difficulty 1/2/3 all correct) ‚Üí Indicator Score= 1√ó1 + 2√ó1 + 3√ó1 = 6",
        "item2": "Indicator B (weight 2): 2 cases (difficulty 2/3 all correct) ‚Üí Indicator Score= 2√ó1 + 3√ó1 = 5",
        "item3": "Total Score = 6√ó4 + 5√ó2 = 34",
        "item4": "Max Score = (1+2+3)√ó4 + (2+3)√ó2 = 34",
        "item5": "Final Score = 34/34√ó100 = 100 points"
      }
    },
    "rule_id": "Rule ID"
  },
  "actions": {
    "zh": "Chinese Eval",
    "en": "English Eval",
    "searchPlaceholder": "Search model name",
    "export": "Export",
    "back": "Back to List",
    "toggle_language": "English",
    "expand": "View All",
    "collapse": "Collapse",
    "submit_report": "Submit Your Report",
    "close": "Close"
  },
  "compare": {
    "toggle_compare_mode": "Model Compare",
    "exit_compare_mode": "Exit Compare",
    "selected_models_count": "Selected {{count}}/5 models",
    "start_compare": "Start Compare ({{count}} models)",
    "compare_mode_tip": "Compare mode enabled: Click table rows or checkboxes to select models, up to 5 models for comparison",
    "checkbox_tooltip": {
      "select": "Select this model for comparison",
      "deselect": "Deselect this model",
      "max_reached": "Maximum 5 models can be selected for comparison"
    },
    "minimum_models_required": "Please select at least 2 models for comparison",
    "back_to_ranking": "Back to Leaderboard",
    "analysis_title": "Model Comparison Analysis",
    "compare_models_label": "Compare Models:",
    "add_model": "Add Model",
    "max_models_reached": "Maximum 5 models can be selected for comparison",
    "switch_language": "Switch Language",
    "loading_data": "Loading comparison data...",
    "chart_types": {
      "radar": "Dimension Capability Radar Chart",
      "bar": "Detailed Score Bar Chart",
      "heatmap": "Heatmap"
    },
    "capability_filter": {
      "all": "All Dimensions",
      "sql_optimization": "SQL Optimization",
      "dialect_conversion": "Dialect Conversion",
      "sql_understanding": "SQL Understanding"
    },
    "chart_titles": {
      "capability_comparison": "Capability Comparison Visualization",
      "detailed_comparison": "Detailed Data Comparison",
      "model_capability_radar": "Model Capability Radar Chart",
      "indicator_comparison": "Indicator Capability Comparison",
      "model_indicator_heatmap": "Model Indicator Heatmap"
    },
    "export_data": "Export Data",
    "evaluation_item": "Evaluation Item",
    "model_comparison_data": "Model Comparison Data",
    "all_models_unavailable": "All selected models are unavailable in {{month}}, redirected to leaderboard",
    "insufficient_models_available": "Only {{available}} models available in {{month}}, insufficient for comparison, redirected to leaderboard",
    "some_models_unavailable": "{{count}} models are unavailable in {{month}}, comparing with remaining {{available}} models",
    "month_switch_error": "Error occurred while switching month, please try again later",
    "month_data_not_found": "Data for {{month}} does not exist",
    "average_score_label": "Avg Score",
    "rank_label": "Rank",
    "score_unit": "pts",
    "indicator_fallback": "Indicator",
    "indicator_comparison_suffix": " Indicator Comparison",
    "heatmap_suffix": " Heatmap",
    "high_score": "High",
    "low_score": "Low"
  },
  "submission_guide": {
    "title": "Submission Guidelines",
    "intro": "We welcome community contributions! To ensure the quality and consistency of the leaderboard, please follow these guidelines when submitting your model's evaluation report.",
    "cta_button": "Contribute on GitHub",
    "req_title": "Submission Requirements",
    "req1": "Report Integrity: All three reports generated by the evaluator must be submitted in full and the evaluation time (model score report, case run report, evaluation process report) must be stated in the PR.",
    "req2": "Complete Model Configuration: The model configuration in `llm_config.py` under `TARGET_LLM_CONFIG` must be complete, including all display-related fields.",
    "req3": "Standard Datasets: If using our standard dataset, you must perform a full evaluation across all three dimensions and all indicators.",
    "req4": "Custom Datasets: If using a custom dataset, ensure it follows the required format. A full evaluation across all three dimensions and indicators is still required.",
    "req5": "No Duplicate Models: We do not accept model evaluation reports that are already on the ranking list for the month you are evaluating the model.",
    "req6": "Further Reading: For detailed instructions on using the evaluator and other information, please refer to the `README.md` in our GitHub repository."
  },
  "footer": {
    "copyright": "¬© Copyright 2025 Shanghai Action Information Technology Co., Ltd. All rights reserved.",
    "beian": "Ê≤™ICPÂ§á12003970Âè∑-5"
  },
  "config": {
    "language": "üåê Language"
  },
  "log_info": {
    "title": "Evaluation Process",
    "select_file": "Select Log File",
    "no_logs_found": "No log files found for this dimension.",
    "no_logs_for_dimension": "No evaluation process log data found for this dimension.",
    "logContent": "Log Content"
  },
  "indicator_ranking": {
    "title": "Model Indicator Score Ranking",
    "dimension_filter": "Dimension Filter",
    "indicator_filter": "Indicator Filter",
    "select_indicator": "Select Indicator",
    "available_indicators": "Available Indicators",
    "current_indicator": "Current Indicator",
    "table_title": "Indicator Ranking",
    "export_csv": "Export CSV",
    "total_models": "Total Models",
    "highest_score": "Highest Score",
    "average_score": "Average Score",
    "no_data": "No Data Available",
    "compare_models": "Compare Models",
    "select_models": "Select Models",
    "start_compare": "Start Compare"
  },
  "dialect_conversion": {
    "logical_equivalence": "Logical Equivalence",
    "syntax_error_detection": "Syntax Error Detection",
    "big_sql_conversion": "Big SQL Conversion",
    "China-made_database": "China-made Database"
  },
  "sql_optimization": {
    "logical_equivalence": "Logical Equivalence",
    "optimization_depth": "Optimization Depth",
    "rule_adherence": "Rule Adherence",
    "syntax_error_detection": "Syntax Error Detection"
  },
  "sql_understanding": {
    "execution_accuracy": "Execution Accuracy",
    "explain_detection": "Explain Detection",
    "sql_identification": "Type Identification",
    "syntax_error_detection": "Syntax Error Detection"
  },
  "log_file": {
    "dialect_conversion": {
      "logical_equivalence": "Logical Equivalence",
      "syntax_error_detection": "Syntax Error Detection",
      "big_sql_conversion": "Big SQL Conversion",
      "China-made_database": "China-made Database"
    },
    "sql_optimization": {
      "logical_equivalence": "Logical Equivalence",
      "optimization_depth": "Optimization Depth",
      "rule_adherence": "Rule Adherence",
      "syntax_error_detection": "Syntax Error Detection"
    },
    "sql_understanding": {
      "execution_accuracy": "Execution Accuracy",
      "explain_detection": "Explain Detection",
      "sql_identification": "Type Identification",
      "syntax_error_detection": "Syntax Error Detection"
    }
  }
}